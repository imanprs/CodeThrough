---
title: "A Janitor for Data Cleaning and Examination"
author: "Iman Parsa"
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    highlight: tango
    self_contained: false
    includes:
      after_body: footer.html
---

<style>
body {
text-align: justify}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# janitor Package

Data, especially secondary data, can be very messy and difficult to work with unless we perform some transformations and modifications to change the data into a clean dataset that we can work with. In fact **data cleaning can take up to 80% of the time of a data scientist**^[https://towardsdatascience.com/data-cleaning-with-r-and-the-tidyverse-detecting-missing-values-ea23c519bc62]. The data cleaning process usually requires taking a look at the data and removing outliers, etc. Above that, the original data is rarely in the format that you want for the analysis. janitor is a package with simple functions for cleaning and examining data.

# Messy data example

The example we are using is an [excel sheet](https://github.com/imanprs/CodeThrough/raw/main/dirty_data.xlsx) which looks like this.

![](https://github.com/imanprs/CodeThrough/raw/main/DataSS.jpg)
It is clear that we are not given a clean dataset to work with. Now we load the data as a dataframe in R.

```{r, results='hide', message=F}
library(dplyr)
library(readxl)
library(httr)
library(pander)

url <- "https://github.com/imanprs/CodeThrough/raw/main/dirty_data.xlsx"
GET(url,write_disk(tf<-tempfile(fileext = ".xlsx")))
messy_data <- read_excel(tf)
```

This is what the dataset looks like in R (only first 8 rows are shown):

```{r}
pander(messy_data[1:8,])
```


# Cleaning

The issues we would want to take care of are as follows:

- Column names;
- Rows and columns containing no data;
- Dates that are stored as numbers;
- Inconsistent data in “Certification” columns

## Variable names
The function `clean_names()` changes the variable names into clean names that are unique and consist only of the _ character, numbers, and letters. We can set the capitalization with `case` parameter. The default, "snake", separates words in the names with an _ character, e.g. first_name. "small_camel" makes names like firstName.

```{r,  results='hide', message=F}
library(janitor)
data <- messy_data %>%
  clean_names()
```

Let's compare the variable names in the two datasets.

```{r}
names(messy_data)
names(data)
```

Another alternative is to use `make_clean_names()` function as the name repair setting of `read_excel()` function. In that case, we would load the data with the following code:

```{r}
data <- read_excel(tf,.name_repair = make_clean_names)
names(data)
```

Note that the excel sheet contains three columns with the same name (Certificate). In the first method, `clean_names()` appends the column number at the end to differentiate these variables. In the second method, using `make_clean_names()`, after cleaning the first name, any repetitions would take numbers 2, 3, ... at the end.

## Empty rows and columns
We need to get rid of empty rows and columns. `remove_empty` is the function for that.
```{r}
data <- remove_empty(data, c("rows", "cols"))
data
```


## Dates
Dates are stored as numeric in the excel sheet, so let's fix that and convert it to date. The function `excel_numeric_to_date()` makes it easy to do.
```{r}
data <- mutate(data,hire_date = excel_numeric_to_date(hire_date))
data
```

## Certificate columns
We have three columns for certificates in the excel file, one of which was removed as an empty column. We can use the function `coalesce` from `dplyr` package to form a single variable for certificates. This function finds the first non-missing value at each position, so if the first column is empty, the value in the second column would be considered as the certificate.
```{r}
data <- mutate(data, cert = coalesce(certification, certification_2)) %>%
  select(-certification, -certification_2)
data
```
We used `select` to drop the original columns for certificates.

# Examining data
After doing initial data cleaning as above, we may want to take a look at the data to diagnose any irregualarities that we do not want.

## Find duplicates
The package has a function `get_dupes()` that we can use to identify duplicate records which we may want to examine and, for instance, decide if they need to be kept in the data. This function brings the rows that have identical values in the variables that we specify. In this example, we can find teachers that are listed more than once, as follows.

```{r}
get_dupes(data, contains("name"))
```

Or, we can find teachers with the same last name who were hired on the same date.
```{r}
get_dupes(data, last_name, hire_date)
```

## Tabulating data
Tabulating data is a good way to examine the data for irregularities. `tabyl()` is the function that tabulates one to three variables. The function is similar to `table()` but is more flexible.

For instance, we can tabulate a single variable, as follows, to see the distribution of subjects in the data.

```{r}
data %>%
  tabyl(subject)
```

We can also tabulate two variables. For instance, let's see how many full-time and part-time teachers with different statuses were hired since 1950.
```{r}
data %>%
  filter(hire_date > as.Date("1950-01-01")) %>%
  tabyl(employee_status, full_time)
```

The function works for three variables as well.
```{r}
data %>%
  tabyl(full_time, subject, employee_status, show_missing_levels = FALSE)
```

The package also includes `adorn_` functions that can be used to "dress up" these tables and generate tables for quick reporting. In what follows, we use these functions and pipe the final output to `knitr::kable()` to generate the table below.


```{r}
data %>%
  tabyl(employee_status, full_time) %>%
  adorn_totals("row") %>%
  adorn_percentages("row") %>%
  adorn_pct_formatting() %>%
  adorn_ns() %>%
  adorn_title("combined") %>%
  knitr::kable()
```
